# Base configuration for Sparse Attention experiments
# Architecture defined by explicit block counts, not size presets

# Model Architecture
model:
  dim: 512
  num_heads: 8
  num_kv_heads: null  # null = same as num_heads (MHA)
  head_dim: null      # null = dim // num_heads
  ffn_multiplier: 4.0
  vocab_size: 50257   # GPT-2 tokenizer
  max_seq_len: 2048
  dropout: 0.0
  attn_dropout: 0.0
  bias: false
  norm_type: "rmsnorm"
  norm_eps: 1.0e-6
  rope_base: 10000.0
  tie_embeddings: true
  
  # Block composition (explicit control)
  # Pattern: [n_standard_blocks] -> [n_sparse_blocks]
  # Total depth = n_standard + n_sparse
  n_standard_blocks: 4   # Normal causal attention blocks
  n_sparse_blocks: 4     # HyperGraph sparse attention blocks
  
  # Sparse attention settings
  num_hyper_nodes: 4     # K timelines per head

# Training
training:
  learning_rate: 3.0e-4
  min_learning_rate: 1.0e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1.0e-8
  max_grad_norm: 1.0
  
  num_epochs: 10
  warmup_steps: 0
  warmup_ratio: 0.1
  lr_scheduler: "cosine"
  
  batch_size: 8
  gradient_accumulation_steps: 1
  
  mixed_precision: true
  bf16: false
  
  # Checkpointing
  save_every_n_steps: 1000
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 3
  checkpoint_dir: "checkpoints"
  
  # Logging
  log_every_n_steps: 10
  eval_every_n_steps: 500
  wandb_project: null
  wandb_run_name: null

# Data
data:
  max_seq_len: 2048
  num_workers: 4
